{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c589ddec-e969-4ae1-ad24-2a8177137412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438841,) [ -7.  -9. -10. ...   5.   6.  13.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DST Index Data\n",
    "def load_dst_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "    data.set_index(\"ds\", inplace=True)\n",
    "    return data\n",
    "\n",
    "# Load DST dataset (Fine-tuning data)\n",
    "dst_data = load_dst_data(\"dst_data_1975_2025.csv\")\n",
    "\n",
    "# Prepare Data for LSTM Model\n",
    "dst_values = dst_data[\"y\"].values\n",
    "\n",
    "print(dst_values.shape, dst_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21be78f6-f166-4142-92f6-026a7d8bac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sliding window data preparation, One of Time Series Cross-Validation (TSCV)\n",
    "# Sliding Window Cross-Validation (for more accurate measurement of one model)\n",
    "# NOTE: you can use other options: Expanding Window Cross-Validation, Blocked Cross-Validation, ... for TSCV\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size, model_type):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx: idx + self.window_size]\n",
    "        y = self.data[idx + self.window_size]  # assuming DST is the first column\n",
    "        # return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        if self.model_type == \"CNN1D\":\n",
    "            return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32)\n",
    "        else:\n",
    "            return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74189f59-773c-464e-8dc2-0bebd1d3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(data, train_window_size, val_window_size):\n",
    "    total_size = len(data)\n",
    "    splits = []\n",
    "    for start in range(0, total_size - train_window_size - val_window_size, val_window_size):\n",
    "        train_indices = np.arange(start, start + train_window_size)\n",
    "        val_indices = np.arange(start + train_window_size, start + train_window_size + val_window_size)\n",
    "        splits.append((train_indices, val_indices))\n",
    "    return splits\n",
    "\n",
    "def create_dataloader_from_indices(data, indices, window_size, batch_size=64, model_type=\"LSTM\"):\n",
    "    subset_data = data[indices]\n",
    "    dataset = TimeSeriesDataset(subset_data, window_size, model_type)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ff0cff-e792-4991-a0bd-baf0966a7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Efficient Model until now\n",
    "class RECENT_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RECENT_Model, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(1, 512, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(512 * 2, 256, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(256 * 2, 128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x[:, -1, :])  # Get the last hidden state\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "# LSTM Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# LSTM Model Definition\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# BiLSTM Model Definition\n",
    "class BiLSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, output_size=1, num_layers=2, dropout=0.2):\n",
    "        super(BiLSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Double hidden size for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# GRU Model Definition\n",
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(GRU_Model, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_hidden_state = gru_out[:, -1, :]\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# 1D CNN Model Definition\n",
    "class CNN1D_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, window_size=24):\n",
    "        super(CNN1D_Model, self).__init__()\n",
    "        \n",
    "        # 1D Convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * (window_size // 8), 128)  # Adjusting size after pooling\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 + ReLU + Pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 + ReLU + Pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Apply conv3 + ReLU + Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        x = self.dropout(x)  # Apply dropout for regularization\n",
    "        x = self.fc2(x)  # Final output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b7b781-ebe0-497f-9210-a92936566757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop_count = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.early_stop_count = 0\n",
    "        else:\n",
    "            self.early_stop_count += 1\n",
    "        return self.early_stop_count >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bf6551-d24e-4d28-bcaf-3f4d1695f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, model_type, fold, window_size, lr, batch_size, best_metric_value, checkpoint_dir=\"checkpoints\", is_early_stopped=0):\n",
    "    # Create model-specific directory for checkpoints if not exists & Save the checkpoint along with best_metric_value.\n",
    "    model_checkpoint_dir = os.path.join(checkpoint_dir, model_type)\n",
    "    if not os.path.exists(model_checkpoint_dir):\n",
    "        os.makedirs(model_checkpoint_dir)\n",
    "\n",
    "    # Format the checkpoint filename using the hyperparameters\n",
    "    checkpoint_file = f\"checkpoints/{model_type}/window{window_size}_lr{lr:.5f}_batch{batch_size}_fold{fold}_checkpoint.pth\"\n",
    "    if is_early_stopped:\n",
    "        checkpoint_file = f\"checkpoints/{model_type}/window{window_size}_lr{lr:.5f}_batch{batch_size}_fold{fold}_completed.pth\"\n",
    "        \n",
    "    # Save the checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'best_metric_value': best_metric_value,  # Save the best metric value\n",
    "    }, checkpoint_file)\n",
    "    \n",
    "    print(f\"Checkpoint for {model_type} saved at fold {fold + 1}, epoch {epoch}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, model_type, fold, window_size, lr, batch_size, checkpoint_dir=\"checkpoints\"):\n",
    "\n",
    "    # Define checkpoint filename with formatted hyperparameters\n",
    "    checkpoint_file = f\"checkpoints/{model_type}/window{window_size}_lr{lr:.5f}_batch{batch_size}_fold{fold}_checkpoint.pth\"\n",
    "\n",
    "    # Check if checkpoint exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "        best_metric_value = checkpoint['best_metric_value']  # Load best_metric_value\n",
    "        \n",
    "        print(f\"Checkpoint for {model_type} loaded at fold {fold + 1}. Resuming from epoch {epoch}. Current Best Metric Value: {best_metric_value}\")\n",
    "        return model, optimizer, epoch, train_loss, val_loss, best_metric_value\n",
    "    else:\n",
    "        print(f\"No checkpoint found for {model_type} at fold {fold + 1}. Starting from scratch.\")\n",
    "        return model, optimizer, 0, None, None, float('inf') # Start with inf for best_metric_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454d4fc7-3ecc-4bf0-84e5-c1c95f5757e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def train_with_sliding_window(data, window_size, train_window_size, val_window_size, lr=1e-3, batch_size=64, num_epochs=100, model_type=\"LSTM\"):\n",
    "    splits = sliding_window_split(data, train_window_size, val_window_size)\n",
    "\n",
    "    val_losses = []\n",
    "    mae_scores = []\n",
    "    best_model = None\n",
    "    best_model_info = {}  # Store information about the best model\n",
    "    best_metric_value = float('inf')  # Initialize with infinity for best model selection\n",
    "    best_model_info[\"best_val_loss\"] = best_metric_value\n",
    "\n",
    "    # Store information about all models for comparison (not saving them yet)\n",
    "    all_models_info = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(splits):\n",
    "        print(f\"Training fold {fold + 1}/{len(splits)}...\")\n",
    "\n",
    "        # TensorBoard writer - Separate log directories for each fold and model type\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{model_type}/window{window_size}_lr{lr}_batch{batch_size}_fold{fold}\")\n",
    "\n",
    "        train_loader = create_dataloader_from_indices(data, train_indices, window_size, batch_size, model_type)\n",
    "        val_loader = create_dataloader_from_indices(data, val_indices, window_size, batch_size, model_type)\n",
    "\n",
    "        # Model selection\n",
    "        if model_type == \"LSTM\":\n",
    "            model = LSTM_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"BiLSTM\":\n",
    "            model = BiLSTM_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"GRU\":\n",
    "            model = GRU_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"RECENT\":\n",
    "            model = RECENT_Model()  # 1 input feature for DST values        \n",
    "        elif model_type == \"CNN1D\":\n",
    "            model = CNN1D_Model(window_size=window_size)  # 1 input feature for DST values        \n",
    "        else:\n",
    "            return print(\"Unknown model\")\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "\n",
    "        if os.path.exists(f\"checkpoints/{model_type}/window{window_size}_lr{lr:.5f}_batch{batch_size}_fold{fold}_completed.pth\"):\n",
    "            print(\"That Model is completed, so go into next step...\")\n",
    "            continue\n",
    "        # Checkpoint filename for each fold and model\n",
    "        start_epoch = 0\n",
    "        model, optimizer, start_epoch, train_loss, _, best_metric_value = load_checkpoint(model, optimizer, model_type, fold, window_size, lr, batch_size)\n",
    "\n",
    "        # Initialize variables for val_loss and mae before training\n",
    "        val_loss = None\n",
    "        mae = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}...\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_batch)\n",
    "                output = output.reshape(-1)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Avoid division by zero if no training steps are completed\n",
    "            if len(train_loader) > 0:\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_true = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in val_loader:\n",
    "                    output = model(x_batch)\n",
    "                    output = output.reshape(-1)\n",
    "                    loss = criterion(output, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "                    all_preds.append(output.cpu().numpy())\n",
    "                    all_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Avoid division by zero if no validation steps are completed\n",
    "            if len(val_loader) > 0:\n",
    "                val_loss /= len(val_loader)\n",
    "\n",
    "            # MAE Calculation - Handle case if no predictions are made\n",
    "            if len(all_preds) > 0 and len(all_true) > 0:\n",
    "                all_preds = np.concatenate(all_preds, axis=0)\n",
    "                all_true = np.concatenate(all_true, axis=0)\n",
    "                mae = mean_absolute_error(all_true, all_preds)\n",
    "            else:\n",
    "                mae = np.nan  # Handle case where no validation predictions are made\n",
    "\n",
    "            # TensorBoard Logging\n",
    "            writer.add_scalar('Train Loss', train_loss, epoch + 1)\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch + 1)\n",
    "            writer.add_scalar('MAE', mae, epoch + 1)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if early_stopping(val_loss):\n",
    "                print(\"Early stopping triggered!\")\n",
    "                save_checkpoint(model, optimizer, epoch + 1, train_loss, val_loss, model_type, fold, window_size, lr, batch_size, best_metric_value, is_early_stopped=1)\n",
    "                break\n",
    "            \n",
    "            # Save checkpoint\n",
    "            save_checkpoint(model, optimizer, epoch + 1, train_loss, val_loss, model_type, fold, window_size, lr, batch_size, best_metric_value)\n",
    "\n",
    "            # Track the best model based on validation loss or MAE\n",
    "            if val_loss < best_metric_value:  # Use avg_val_loss as metric\n",
    "                best_metric_value = val_loss  # Choose based on validation loss\n",
    "                best_model = model.state_dict()  # Save the best model weights\n",
    "\n",
    "                # Save the best model information (hyperparameters, validation loss, MAE, etc.)\n",
    "                best_model_info = {\n",
    "                    \"model_type\": model_type,\n",
    "                    \"window_size\": window_size,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"fold\": fold,\n",
    "                    \"best_val_loss\": best_metric_value,\n",
    "                    \"mae\": mae,\n",
    "                    \"epochs_trained\": epoch + 1,\n",
    "                    \"model_weights\": f\"best_model_{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}.pth\"\n",
    "                }\n",
    "        \n",
    "        # Save the comprehensive model info for each fold for later comparison\n",
    "        if val_loss is not None and mae is not None:\n",
    "            model_info = {\n",
    "                \"model_type\": model_type,\n",
    "                \"window_size\": window_size,\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"fold\": fold,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"mae\": mae\n",
    "            }\n",
    "            all_models_info.append(model_info)\n",
    "            val_losses.append(val_loss)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "    # Calculate and log the average validation loss and MAE across all folds\n",
    "    avg_val_loss = np.mean(val_losses) if len(val_losses) > 0 else np.nan\n",
    "    avg_mae = np.mean(mae_scores) if len(mae_scores) > 0 else np.nan\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}, Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "    # Log average values to TensorBoard\n",
    "    writer.add_scalar('Average Validation Loss', avg_val_loss, num_epochs + 1)\n",
    "    writer.add_scalar('Average MAE', avg_mae, num_epochs + 1)\n",
    "\n",
    "    # Save only the best model's information and weights\n",
    "    if best_model is not None:\n",
    "        print(f\"Saving the best model with validation loss: {best_metric_value:.4f}\")\n",
    "        torch.save(best_model, f\"best_model.pth\")\n",
    "\n",
    "        # Save the best model information in a JSON file\n",
    "        with open(\"best_model.json\", 'w') as f:\n",
    "            json.dump(best_model_info, f, indent=4)\n",
    "        \n",
    "        print(f\"Best model information saved to best_model.json\")\n",
    "    \n",
    "    writer.close()  # Close TensorBoard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e8394-f230-4d05-acd1-469f3e28a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with window_size=24, lr=0.01, batch_size=1024\n",
      "Training fold 1/3...\n",
      "That Model is completed, so go into next step...\n",
      "Training fold 2/3...\n",
      "That Model is completed, so go into next step...\n",
      "Training fold 3/3...\n",
      "That Model is completed, so go into next step...\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=24, lr=0.01, batch_size=128\n",
      "Training fold 1/3...\n",
      "That Model is completed, so go into next step...\n",
      "Training fold 2/3...\n",
      "Checkpoint for LSTM loaded at fold 2. Resuming from epoch 2. Current Best Metric Value: 414.81381661940594\n",
      "Epoch 3/100...\n",
      "Epoch 3/100, Train Loss: 0.0000, Validation Loss: 414.8138, MAE: 13.9898\n",
      "Checkpoint for LSTM saved at fold 2, epoch 3\n",
      "Epoch 4/100...\n",
      "Epoch 4/100, Train Loss: 0.0000, Validation Loss: 414.8138, MAE: 13.9898\n",
      "Checkpoint for LSTM saved at fold 2, epoch 4\n",
      "Epoch 5/100...\n",
      "Epoch 5/100, Train Loss: 0.0000, Validation Loss: 414.8138, MAE: 13.9898\n",
      "Checkpoint for LSTM saved at fold 2, epoch 5\n",
      "Epoch 6/100...\n"
     ]
    }
   ],
   "source": [
    "# number of fold\n",
    "num_fold = 3\n",
    "val_window_size = len(dst_values) // (8 + 2 * num_fold) * 2  # Size of validation window\n",
    "train_window_size = val_window_size * 4  # Size of training window, train: 80%, val: 20% of each fold\n",
    "\n",
    "# Hyperparameter tuning\n",
    "window_sizes = [24, 48, 96]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [512, 64, 8]\n",
    "# Model List\n",
    "model_list = [\"LSTM\", \"BiLSTM\", \"GRU\", \"CNN1D\", \"RECENT\"]\n",
    "\n",
    "for model_type in model_list:\n",
    "        for window_size in window_sizes:\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    print(f\"Training with window_size={window_size}, lr={lr}, batch_size={batch_size}\")\n",
    "                    train_with_sliding_window(dst_values, window_size, train_window_size, val_window_size, lr=lr, batch_size=batch_size, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d5010-e93a-4706-93b9-527a2bc3c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
