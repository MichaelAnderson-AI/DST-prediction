{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c589ddec-e969-4ae1-ad24-2a8177137412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438841,) [ -7.  -9. -10. ...   5.   6.  13.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DST Index Data\n",
    "def load_dst_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "    data.set_index(\"ds\", inplace=True)\n",
    "    return data\n",
    "\n",
    "# Load DST dataset (Fine-tuning data)\n",
    "dst_data = load_dst_data(\"dst_data_1975_2025.csv\")\n",
    "\n",
    "# Prepare Data for LSTM Model\n",
    "dst_values = dst_data[\"y\"].values\n",
    "\n",
    "print(dst_values.shape, dst_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21be78f6-f166-4142-92f6-026a7d8bac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sliding window data preparation\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx: idx + self.window_size]\n",
    "        y = self.data[idx + self.window_size]  # assuming DST is the first column\n",
    "        # return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74189f59-773c-464e-8dc2-0bebd1d3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(data, train_window_size, val_window_size):\n",
    "    total_size = len(data)\n",
    "    splits = []\n",
    "    for start in range(0, total_size - train_window_size - val_window_size, val_window_size):\n",
    "        train_indices = np.arange(start, start + train_window_size)\n",
    "        val_indices = np.arange(start + train_window_size, start + train_window_size + val_window_size)\n",
    "        splits.append((train_indices, val_indices))\n",
    "    return splits\n",
    "\n",
    "def create_dataloader_from_indices(data, indices, window_size, batch_size=64):\n",
    "    subset_data = data[indices]\n",
    "    dataset = TimeSeriesDataset(subset_data, window_size)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ff0cff-e792-4991-a0bd-baf0966a7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(1, 512, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(512 * 2, 256, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(256 * 2, 128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x[:, -1, :])  # Get the last hidden state\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b7b781-ebe0-497f-9210-a92936566757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop_count = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.early_stop_count = 0\n",
    "        else:\n",
    "            self.early_stop_count += 1\n",
    "        return self.early_stop_count >= self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454d4fc7-3ecc-4bf0-84e5-c1c95f5757e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sliding_window(data, window_size, train_window_size, val_window_size, lr=1e-3, batch_size=64, num_epochs=50):\n",
    "    print(\"OK1\")\n",
    "    splits = sliding_window_split(data, train_window_size, val_window_size)\n",
    "    print(\"OK2\")\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=f\"runs/BiLSTM_window{window_size}_lr{lr}_batch{batch_size}\")\n",
    "    print(\"OK3\")\n",
    "    val_losses = []\n",
    "    mae_scores = []\n",
    "    print(\"OK4\")\n",
    "    for fold, (train_indices, val_indices) in enumerate(splits):\n",
    "        print(f\"Training fold {fold + 1}/{len(splits)}...\")\n",
    "\n",
    "        train_loader = create_dataloader_from_indices(data, train_indices, window_size, batch_size)\n",
    "        val_loader = create_dataloader_from_indices(data, val_indices, window_size, batch_size)\n",
    "        print(\"OK5\")\n",
    "        model = BidirectionalLSTM()\n",
    "        print(\"OK6\")\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "        print(\"OK7\")\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"OK8\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            print(\"OK9\")\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                if i % 1000 == 1:\n",
    "                    print(i)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_batch)\n",
    "                output = output.reshape(-1)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            print(train_loss)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_true = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in val_loader:\n",
    "                    output = model(x_batch)\n",
    "                    output = output.reshape(-1)\n",
    "                    loss = criterion(output, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "                    all_preds.append(output.cpu().numpy())\n",
    "                    all_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            # MAE Calculation\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            all_true = np.concatenate(all_true, axis=0)\n",
    "            mae = mean_absolute_error(all_true, all_preds)\n",
    "\n",
    "            # TensorBoard Logging\n",
    "            writer.add_scalar('Train Loss', train_loss, epoch + 1)\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch + 1)\n",
    "            writer.add_scalar('MAE', mae, epoch + 1)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            # if early_stopping(val_loss):\n",
    "            #     print(\"Early stopping triggered!\")\n",
    "            #     break\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}, Average MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    writer.close()  # Close TensorBoard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18e8394-f230-4d05-acd1-469f3e28a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438841 48\n"
     ]
    }
   ],
   "source": [
    "num_fold = 5\n",
    "# Hyperparameter tuning\n",
    "window_size = 48\n",
    "learning_rates = 1e-4\n",
    "batch_size = 64\n",
    "val_window_size = len(dst_values) // (8 + 2 * num_fold) * 2  # Size of validation window\n",
    "train_window_size = val_window_size * 4  # Size of training window\n",
    "\n",
    "print(len(dst_values), window_size)\n",
    "\n",
    "# for window_size in window_sizes:\n",
    "#     for lr in learning_rates:\n",
    "#         for batch_size in batch_sizes:\n",
    "#             print(f\"Training with window_size={window_size}, lr={lr}, batch_size={batch_size}\")\n",
    "#             train_with_sliding_window(dst_values, window_size, train_window_size, val_window_size, lr=lr, batch_size=batch_size, model_type=\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d5010-e93a-4706-93b9-527a2bc3c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK1\n",
      "OK2\n",
      "OK3\n",
      "OK4\n",
      "Training fold 1/5...\n",
      "OK5\n",
      "OK6\n",
      "OK7\n",
      "OK8\n",
      "OK9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_with_sliding_window(dst_values, window_size, train_window_size, val_window_size, lr=learning_rates, batch_size=batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4a8f3-1765-4f9a-b28b-529478a521ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
