{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c589ddec-e969-4ae1-ad24-2a8177137412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438841,) [ -7.  -9. -10. ...   5.   6.  13.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DST Index Data\n",
    "def load_dst_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "    data.set_index(\"ds\", inplace=True)\n",
    "    return data\n",
    "\n",
    "# Load DST dataset (Fine-tuning data)\n",
    "dst_data = load_dst_data(\"dst_data_1975_2025.csv\")\n",
    "\n",
    "# Prepare Data for LSTM Model\n",
    "dst_values = dst_data[\"y\"].values\n",
    "\n",
    "print(dst_values.shape, dst_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21be78f6-f166-4142-92f6-026a7d8bac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sliding window data preparation, One of Time Series Cross-Validation (TSCV)\n",
    "# Sliding Window Cross-Validation (for more accurate measurement of one model)\n",
    "# NOTE: you can use other options: Expanding Window Cross-Validation, Blocked Cross-Validation, ... for TSCV\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size, model_type):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx: idx + self.window_size]\n",
    "        y = self.data[idx + self.window_size]  # assuming DST is the first column\n",
    "        # return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        if self.model_type == \"LSTM\":\n",
    "            return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32)\n",
    "        else:\n",
    "            return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74189f59-773c-464e-8dc2-0bebd1d3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(data, train_window_size, val_window_size):\n",
    "    total_size = len(data)\n",
    "    splits = []\n",
    "    for start in range(0, total_size - train_window_size - val_window_size, val_window_size):\n",
    "        train_indices = np.arange(start, start + train_window_size)\n",
    "        val_indices = np.arange(start + train_window_size, start + train_window_size + val_window_size)\n",
    "        splits.append((train_indices, val_indices))\n",
    "    return splits\n",
    "\n",
    "def create_dataloader_from_indices(data, indices, window_size, batch_size=32, model_type=\"LSTM\"):\n",
    "    subset_data = data[indices]\n",
    "    dataset = TimeSeriesDataset(subset_data, window_size, model_type)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ff0cff-e792-4991-a0bd-baf0966a7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Definition\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to the prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Dropout layer to avoid overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout and pass through fully connected layer\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# BiLSTM Model Definition\n",
    "class BiLSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=1, num_layers=1):\n",
    "        super(BiLSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Double hidden size for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define the Model (Bidirectional LSTM)\n",
    "class RECENT_Model(nn.Module):\n",
    "    def __init__(self, window_size=24):\n",
    "        super(RECENT_Model, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(window_size, 512, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(512 * 2, 256, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm3 = nn.LSTM(256 * 2, 128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x[:, -1, :])  # Get the last hidden state\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "# GRU Model Definition\n",
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=1, num_layers=1):\n",
    "        super(GRU_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Double hidden size for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class CNN1D_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, window_size=24):\n",
    "        super(CNN1D_Model, self).__init__()\n",
    "        \n",
    "        # 1D Convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max-pooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(64 * (window_size // 2), 128)  # Adjusting size after pooling\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 + ReLU + Pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 + ReLU + Pooling\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # Apply conv3 + ReLU + Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        x = self.dropout(x)  # Apply dropout for regularization\n",
    "        x = self.fc2(x)  # Final output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b7b781-ebe0-497f-9210-a92936566757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop_count = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.early_stop_count = 0\n",
    "        else:\n",
    "            self.early_stop_count += 1\n",
    "        return self.early_stop_count >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bf6551-d24e-4d28-bcaf-3f4d1695f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, model_type, fold, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create a model-specific checkpoint directory\n",
    "    model_checkpoint_dir = os.path.join(checkpoint_dir, model_type)\n",
    "    if not os.path.exists(model_checkpoint_dir):\n",
    "        os.makedirs(model_checkpoint_dir)\n",
    "\n",
    "    # Save the checkpoint\n",
    "    checkpoint_filename = os.path.join(model_checkpoint_dir, f\"checkpoint_fold{fold + 1}.pth\")\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, checkpoint_filename)\n",
    "    print(f\"Checkpoint for {model_type} saved at fold {fold + 1} (epoch {epoch})\")\n",
    "\n",
    "def load_checkpoint_if_matches(model, optimizer, model_type, fold, window_size, checkpoint_dir=\"checkpoints\"):\n",
    "    # Load checkpoint only if the input size (window_size) matches\n",
    "    model_checkpoint_dir = os.path.join(checkpoint_dir, model_type)\n",
    "    checkpoint_filename = os.path.join(model_checkpoint_dir, f\"checkpoint_fold{fold + 1}.pth\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_filename):\n",
    "        checkpoint = torch.load(checkpoint_filename)\n",
    "        \n",
    "        # For LSTM, BiLSTM, GRU models: Check if lstm1 weights match the window size\n",
    "        if model_type in ['LSTM', 'BiLSTM', 'GRU', 'RECENT']:\n",
    "            if checkpoint['model_state_dict']['lstm1.weight_ih_l0'].shape[1] == window_size:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                epoch = checkpoint['epoch']\n",
    "                train_loss = checkpoint['train_loss']\n",
    "                val_loss = checkpoint['val_loss']\n",
    "                print(f\"Checkpoint loaded for fold {fold + 1} (window_size {window_size})\")\n",
    "                return model, optimizer, epoch, train_loss, val_loss\n",
    "            else:\n",
    "                print(f\"Checkpoint for fold {fold + 1} is incompatible with window_size {window_size}. Starting from scratch.\")\n",
    "                return model, optimizer, 0, None, None\n",
    "        \n",
    "        # For CNN1D model: Check if conv1 weights match the window size (input channels)\n",
    "        elif model_type == 'CNN1D':\n",
    "            if checkpoint['model_state_dict']['conv1.weight'].shape[1] == 1:  # 1 input channel, window_size can be any\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                epoch = checkpoint['epoch']\n",
    "                train_loss = checkpoint['train_loss']\n",
    "                val_loss = checkpoint['val_loss']\n",
    "                print(f\"Checkpoint loaded for fold {fold + 1} (window_size {window_size})\")\n",
    "                return model, optimizer, epoch, train_loss, val_loss\n",
    "            else:\n",
    "                print(f\"Checkpoint for fold {fold + 1} is incompatible with CNN1D model. Starting from scratch.\")\n",
    "                return model, optimizer, 0, None, None\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unknown model type: {model_type}. Starting from scratch.\")\n",
    "            return model, optimizer, 0, None, None\n",
    "    else:\n",
    "        print(f\"No checkpoint found for fold {fold + 1}. Starting from scratch.\")\n",
    "        return model, optimizer, 0, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454d4fc7-3ecc-4bf0-84e5-c1c95f5757e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def train_with_sliding_window(data, window_size, train_window_size, val_window_size, lr=1e-3, batch_size=32, num_epochs=4, model_type=\"LSTM\"):\n",
    "    splits = sliding_window_split(data, train_window_size, val_window_size)\n",
    "    \n",
    "    val_losses = []\n",
    "    mae_scores = []\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')  # Initialize with infinity to ensure the first model will be saved\n",
    "    best_model_info = {}  # To store information about the best model\n",
    "    \n",
    "    for fold, (train_indices, val_indices) in enumerate(splits):\n",
    "        print(f\"Training fold {fold + 1}/{len(splits)}...\")\n",
    "        \n",
    "        # TensorBoard writer\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}\")\n",
    "\n",
    "        train_loader = create_dataloader_from_indices(data, train_indices, window_size, batch_size, model_type)\n",
    "        val_loader = create_dataloader_from_indices(data, val_indices, window_size, batch_size, model_type)\n",
    "\n",
    "        # Model selection\n",
    "        if model_type == \"LSTM\":\n",
    "            model = LSTM_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"BiLSTM\":\n",
    "            model = BiLSTM_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"GRU\":\n",
    "            model = GRU_Model()  # 1 input feature for DST values\n",
    "        elif model_type == \"CNN1D\":\n",
    "            model = CNN1D_Model(window_size=window_size)  # 1 input feature for DST values        \n",
    "        elif model_type == \"RECENT\":\n",
    "            model = RECENT_Model(window_size=window_size)  # 1 input feature for DST values        \n",
    "        else:\n",
    "            return print(\"Unknown model\")\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "\n",
    "        # Checkpoint filename for each fold and model\n",
    "        start_epoch = 0\n",
    "        # Only load checkpoint if the window_size matches\n",
    "        model, optimizer, start_epoch, train_loss, _ = load_checkpoint_if_matches(model, optimizer, model_type, fold, window_size)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}...\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                break\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_batch)\n",
    "                output = output.reshape(-1)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                break\n",
    "\n",
    "            # Avoid division by zero if no training steps are completed\n",
    "            if len(train_loader) > 0:\n",
    "                train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_true = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in val_loader:\n",
    "                    output = model(x_batch)\n",
    "                    output = output.reshape(-1)\n",
    "                    loss = criterion(output, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "                    all_preds.append(output.cpu().numpy())\n",
    "                    all_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Avoid division by zero if no validation steps are completed\n",
    "            if len(val_loader) > 0:\n",
    "                val_loss /= len(val_loader)\n",
    "            \n",
    "            # MAE Calculation\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            all_true = np.concatenate(all_true, axis=0)\n",
    "            mae = mean_absolute_error(all_true, all_preds)\n",
    "\n",
    "            # TensorBoard Logging\n",
    "            writer.add_scalar('Train Loss', train_loss, epoch + 1)\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch + 1)\n",
    "            writer.add_scalar('MAE', mae, epoch + 1)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if early_stopping(val_loss):\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "            \n",
    "            # Save checkpoint\n",
    "            save_checkpoint(model, optimizer, epoch + 1, train_loss, val_loss, model_type, fold)\n",
    "\n",
    "            # Track the best model based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict()  # Save the best model weights\n",
    "\n",
    "                # Save the best model information (hyperparameters, validation loss, MAE, etc.)\n",
    "                best_model_info = {\n",
    "                    \"model_type\": model_type,\n",
    "                    \"window_size\": window_size,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"fold\": fold,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"mae\": mae,\n",
    "                    \"epochs_trained\": epoch + 1,\n",
    "                    \"model_weights\": f\"best_model_{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}.pth\"\n",
    "                }\n",
    "            \n",
    "        # print(\"val_loss\", val_loss)\n",
    "        # val_losses.append(val_loss)\n",
    "        # mae_scores.append(mae)\n",
    "        \n",
    "        # Ensure val_loss is always available before appending\n",
    "        if 'val_loss' in locals():\n",
    "            print(\"val_loss\", val_loss)\n",
    "            val_losses.append(val_loss)\n",
    "        else:\n",
    "            print(\"Warning: No validation loss computed.\")\n",
    "\n",
    "        # Ensure mae is always available before appending\n",
    "        if 'mae' in locals():\n",
    "            print(\"mae\", mae)\n",
    "            mae_scores.append(mae)\n",
    "        else:\n",
    "            print(\"Warning: No mae computed.\")\n",
    "\n",
    "    # Save the best model for this hyperparameter configuration\n",
    "    if best_model is not None:\n",
    "        print(f\"Saving the best model with validation loss: {best_val_loss:.4f}\")\n",
    "        torch.save(best_model, f\"best_model_{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}.pth\")\n",
    "\n",
    "        # Save additional information about the best model\n",
    "        with open(f\"best_model_info_{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}.json\", 'w') as f:\n",
    "            json.dump(best_model_info, f, indent=4)\n",
    "        \n",
    "        print(f\"Best model information saved to best_model_info_{model_type}_window{window_size}_lr{lr}_batch{batch_size}_fold{fold}.json\")\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}, Average MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    writer.close()  # Close TensorBoard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18e8394-f230-4d05-acd1-469f3e28a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with window_size=24, lr=0.01, batch_size=64\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=24, lr=0.01, batch_size=32\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=24, lr=0.01, batch_size=1\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with window_size=48, lr=0.01, batch_size=64\n",
      "Training fold 1/3...\n",
      "Checkpoint for fold 1 is incompatible with window_size 48. Starting from scratch.\n",
      "Epoch 1/4...\n",
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 465.3720, MAE: 13.4839\n",
      "Checkpoint for RECENT saved at fold 1 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 465.3720, MAE: 13.4839\n",
      "Checkpoint for RECENT saved at fold 1 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 465.3720, MAE: 13.4839\n",
      "Checkpoint for RECENT saved at fold 1 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 465.3720, MAE: 13.4839\n",
      "Checkpoint for RECENT saved at fold 1 (epoch 4)\n",
      "val_loss 465.37203825920426\n",
      "mae 13.483863830566406\n",
      "Training fold 2/3...\n",
      "Checkpoint for fold 2 is incompatible with window_size 48. Starting from scratch.\n",
      "Epoch 1/4...\n",
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 416.7233, MAE: 14.0298\n",
      "Checkpoint for RECENT saved at fold 2 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 416.7233, MAE: 14.0298\n",
      "Checkpoint for RECENT saved at fold 2 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 416.7233, MAE: 14.0298\n",
      "Checkpoint for RECENT saved at fold 2 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 416.7233, MAE: 14.0298\n",
      "Checkpoint for RECENT saved at fold 2 (epoch 4)\n",
      "val_loss 416.7233064001744\n",
      "mae 14.029792785644531\n",
      "Training fold 3/3...\n",
      "Checkpoint for fold 3 is incompatible with window_size 48. Starting from scratch.\n",
      "Epoch 1/4...\n",
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 307.1833, MAE: 12.1905\n",
      "Checkpoint for RECENT saved at fold 3 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 307.1833, MAE: 12.1905\n",
      "Checkpoint for RECENT saved at fold 3 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 307.1833, MAE: 12.1905\n",
      "Checkpoint for RECENT saved at fold 3 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 307.1833, MAE: 12.1905\n",
      "Checkpoint for RECENT saved at fold 3 (epoch 4)\n",
      "val_loss 307.18333399892464\n",
      "mae 12.190482139587402\n",
      "Saving the best model with validation loss: 307.1833\n",
      "Best model information saved to best_model_info_RECENT_window48_lr0.01_batch64_fold2.json\n",
      "Average Validation Loss: 396.4262, Average MAE: 13.2347\n",
      "Training with window_size=48, lr=0.01, batch_size=32\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=48, lr=0.01, batch_size=1\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 48)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=24, lr=0.01, batch_size=64\n",
      "Training fold 1/3...\n",
      "No checkpoint found for fold 1. Starting from scratch.\n",
      "Epoch 1/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 481.9822, MAE: 13.7344\n",
      "Checkpoint for CNN1D saved at fold 1 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 481.9822, MAE: 13.7344\n",
      "Checkpoint for CNN1D saved at fold 1 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 481.9822, MAE: 13.7344\n",
      "Checkpoint for CNN1D saved at fold 1 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 481.9822, MAE: 13.7344\n",
      "Checkpoint for CNN1D saved at fold 1 (epoch 4)\n",
      "val_loss 481.98219590965584\n",
      "mae 13.73444938659668\n",
      "Training fold 2/3...\n",
      "No checkpoint found for fold 2. Starting from scratch.\n",
      "Epoch 1/4...\n",
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 415.1768, MAE: 14.0123\n",
      "Checkpoint for CNN1D saved at fold 2 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 415.1768, MAE: 14.0123\n",
      "Checkpoint for CNN1D saved at fold 2 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 415.1768, MAE: 14.0123\n",
      "Checkpoint for CNN1D saved at fold 2 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 415.1768, MAE: 14.0123\n",
      "Checkpoint for CNN1D saved at fold 2 (epoch 4)\n",
      "val_loss 415.1767846905455\n",
      "mae 14.012273788452148\n",
      "Training fold 3/3...\n",
      "No checkpoint found for fold 3. Starting from scratch.\n",
      "Epoch 1/4...\n",
      "Epoch 1/4, Train Loss: 0.0000, Validation Loss: 301.9498, MAE: 12.0742\n",
      "Checkpoint for CNN1D saved at fold 3 (epoch 1)\n",
      "Epoch 2/4...\n",
      "Epoch 2/4, Train Loss: 0.0000, Validation Loss: 301.9498, MAE: 12.0742\n",
      "Checkpoint for CNN1D saved at fold 3 (epoch 2)\n",
      "Epoch 3/4...\n",
      "Epoch 3/4, Train Loss: 0.0000, Validation Loss: 301.9498, MAE: 12.0742\n",
      "Checkpoint for CNN1D saved at fold 3 (epoch 3)\n",
      "Epoch 4/4...\n",
      "Epoch 4/4, Train Loss: 0.0000, Validation Loss: 301.9498, MAE: 12.0742\n",
      "Checkpoint for CNN1D saved at fold 3 (epoch 4)\n",
      "val_loss 301.9498003044907\n",
      "mae 12.074151992797852\n",
      "Saving the best model with validation loss: 301.9498\n",
      "Best model information saved to best_model_info_CNN1D_window24_lr0.01_batch64_fold2.json\n",
      "Average Validation Loss: 399.7029, Average MAE: 13.2736\n",
      "Training with window_size=24, lr=0.01, batch_size=32\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=24, lr=0.01, batch_size=1\n",
      "Training fold 1/3...\n",
      "Checkpoint loaded for fold 1 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 2/3...\n",
      "Checkpoint loaded for fold 2 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Training fold 3/3...\n",
      "Checkpoint loaded for fold 3 (window_size 24)\n",
      "Warning: No validation loss computed.\n",
      "Warning: No mae computed.\n",
      "Average Validation Loss: nan, Average MAE: nan\n",
      "Training with window_size=48, lr=0.01, batch_size=64\n",
      "Training fold 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN1D_Model:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 768]) from checkpoint, the shape in current model is torch.Size([128, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with window_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrain_with_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_window_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_window_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mtrain_with_sliding_window\u001b[1;34m(data, window_size, train_window_size, val_window_size, lr, batch_size, num_epochs, model_type)\u001b[0m\n\u001b[0;32m     41\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Only load checkpoint if the window_size matches\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m model, optimizer, start_epoch, train_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint_if_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n",
      "Cell \u001b[1;32mIn[6], line 44\u001b[0m, in \u001b[0;36mload_checkpoint_if_matches\u001b[1;34m(model, optimizer, model_type, fold, window_size, checkpoint_dir)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN1D\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# 1 input channel, window_size can be any\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m         epoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN1D_Model:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 768]) from checkpoint, the shape in current model is torch.Size([128, 1536])."
     ]
    }
   ],
   "source": [
    "# number of fold\n",
    "num_fold = 3\n",
    "val_window_size = len(dst_values) // (8 + 2 * num_fold) * 2  # Size of validation window\n",
    "train_window_size = val_window_size * 4  # Size of training window, train: 80%, val: 20% of each fold\n",
    "\n",
    "# Hyperparameter tuning\n",
    "# window_sizes = [24, 48, 96]\n",
    "# learning_rates = [3e-3, 1e-3, 3e-4]\n",
    "# batch_sizes = [1, 32]\n",
    "# # Model List\n",
    "# model_list = [\"LSTM\", \"BiLSTM\", \"GRU\", \"CNN1D\"]\n",
    "\n",
    "# test\n",
    "model_list = [\"RECENT\", \"CNN1D\"]\n",
    "window_sizes = [24, 48]\n",
    "learning_rates = [1e-2]\n",
    "batch_sizes = [128, 32, 1]\n",
    "\n",
    "for model_type in model_list:\n",
    "        for window_size in window_sizes:\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    print(f\"Training with window_size={window_size}, lr={lr}, batch_size={batch_size}\")\n",
    "                    train_with_sliding_window(dst_values, window_size, train_window_size, val_window_size, lr=lr, batch_size=batch_size, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d5010-e93a-4706-93b9-527a2bc3c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
